{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from src.calculate_metrics import F1\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://github.com/Devyanshu/image-split-with-overlap/tree/master\n",
    "\n",
    "class BatchesFromSingleImageMask:\n",
    "    def __init__(self, imagefilename, batch_size, maskfilename, kernel_size=(1024, 1024), overlap=0.2, mode='train') -> None:\n",
    "        self.imagefilename = imagefilename\n",
    "        self.batch_size = batch_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.mode = mode\n",
    "\n",
    "        ####################################\n",
    "        # Augmentation here\n",
    "        self.train_transform = A.Compose([\n",
    "        \n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.VerticalFlip(p=1),\n",
    "                    A.HorizontalFlip(p=1),\n",
    "                    A.RandomRotate90(p=1),\n",
    "                    A.Transpose(p=1),\n",
    "                ],\n",
    "                p=0.5,\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        #\n",
    "        ####################################\n",
    "        \n",
    "        big_image = cv2.imread(imagefilename, cv2.IMREAD_COLOR)\n",
    "        big_image = cv2.cvtColor(big_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            big_mask = cv2.imread(maskfilename, cv2.IMREAD_GRAYSCALE)\n",
    "            assert big_image.shape[0] == big_mask.shape[0] and big_image.shape[1] == big_mask.shape[1], \"Image and mask shapes do not match\"\n",
    "            # Transform\n",
    "            big_image, big_mask = self._transform(big_image, big_mask)\n",
    "            \n",
    "            self.masks = list(self._split_image(big_mask, kernel_size, overlap, is_mask=True))\n",
    "            self.big_mask = big_mask.to(device)\n",
    "        elif self.mode == 'test':\n",
    "            big_image = torch.from_numpy(big_image).permute(2, 0, 1)\n",
    "            big_image = big_image / 255.0\n",
    "            \n",
    "        self.orig_shape = big_image.shape[1:]\n",
    "        self.big_image = big_image.to(device)\n",
    "        self.images = list(self._split_image(big_image, kernel_size, overlap))\n",
    "        self.generate_batches()\n",
    "\n",
    "    def _transform(self, image, mask):\n",
    "        self.transform = A.Compose([\n",
    "                # A.RandomRotate90(p=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        \n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image = augmented['image'].float() / 255.0\n",
    "        mask = augmented['mask']\n",
    "        return image, mask\n",
    "    \n",
    "    def _split_image(self, big_image: torch.Tensor, desired_size: tuple=(512, 512), overlap=0.5, is_mask=False):\n",
    "        \"\"\"\n",
    "            Generates the patches of smaller sizes from the big image.\n",
    "            \n",
    "        Args:\n",
    "            big_image (tensor): An image with size bigger the desired size\n",
    "            desired_size (tuple(int, int)): Desired size of the patches\n",
    "            overlap (0 <= float < 1): the precentage of image to be overlapped\n",
    "            is_mask (bool): Different behaviour for mask\n",
    "        Returns:\n",
    "            generator: Generator of ImageContainer objeects\n",
    "        \"\"\"\n",
    "        if is_mask:\n",
    "            big_image = big_image[None, :, :]\n",
    "        img_c, img_h, img_w, = big_image.shape\n",
    "        split_width, split_height = desired_size\n",
    "\n",
    "        def start_points(size, split_size, overlap=0):\n",
    "            points = [0]\n",
    "            stride = int(split_size * (1-overlap))\n",
    "            counter = 1\n",
    "            while True:\n",
    "                pt = stride * counter\n",
    "                if pt + split_size >= size:\n",
    "                    if split_size == size:\n",
    "                        break\n",
    "                    points.append(size - split_size)\n",
    "                    break\n",
    "                else:\n",
    "                    points.append(pt)\n",
    "                counter += 1\n",
    "            return points\n",
    "\n",
    "        X_points = start_points(img_w, split_width, overlap)\n",
    "        Y_points = start_points(img_h, split_height, overlap)\n",
    "\n",
    "        for i in Y_points:\n",
    "            for j in X_points:\n",
    "                yield (i, j)\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        self.batches = []\n",
    "        if self.mode == 'train':\n",
    "            # Train mode\n",
    "            for i in range(0, len(self.images), self.batch_size)[:-1]:\n",
    "                image_stack, mask_stack = [], []\n",
    "                for j in range(self.batch_size):\n",
    "                    image_slice = self._get_image_slice(self.images[i + j]).permute(1, 2, 0).cpu().numpy()\n",
    "                    mask_slice = self._get_mask_slice(self.images[i + j]).cpu().numpy()\n",
    "\n",
    "                    augmented = self.train_transform(image=image_slice, mask=mask_slice)\n",
    "                    image = augmented['image']\n",
    "                    mask = augmented['mask']\n",
    "\n",
    "                    image_stack.append(image)\n",
    "                    mask_stack.append(mask)\n",
    "                self.batches.append([\n",
    "                    torch.stack(image_stack),\n",
    "                    torch.stack(mask_stack)\n",
    "                ])\n",
    "            image_stack, mask_stack = [], []\n",
    "            for j in range(self.batch_size):\n",
    "                image_slice = self._get_image_slice(self.images[len(self.images) - self.batch_size + j]).permute(1, 2, 0).cpu().numpy()\n",
    "                mask_slice = self._get_mask_slice(self.images[len(self.masks) - self.batch_size + j]).cpu().numpy()\n",
    "                \n",
    "                augmented = self.train_transform(image=image_slice, mask=mask_slice)\n",
    "                image = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "\n",
    "                image_stack.append(image)\n",
    "                mask_stack.append(mask)\n",
    "            self.batches.append([\n",
    "                torch.stack(image_stack),\n",
    "                torch.stack(mask_stack)\n",
    "            ])\n",
    "        elif self.mode == 'test':\n",
    "            # Test mode\n",
    "            for i in range(0, len(self.images), self.batch_size)[:-1]:\n",
    "                self.batches.append(\n",
    "                    torch.stack(tuple(self._get_image_slice(self.images[i + j]) for j in range(self.batch_size)))\n",
    "                )\n",
    "            self.batches.append(\n",
    "                torch.stack(tuple(self._get_image_slice(self.images[len(self.images) - self.batch_size + j]) for j in range(self.batch_size)))\n",
    "            )\n",
    "        else:\n",
    "            # Val mode\n",
    "            for i in range(0, len(self.images), self.batch_size)[:-1]:\n",
    "                self.batches.append(\n",
    "                    (torch.stack(tuple(self._get_image_slice(self.images[i + j]) for j in range(self.batch_size))),\n",
    "                    torch.stack(tuple(self._get_mask_slice(self.masks[i + j]) for j in range(self.batch_size))))\n",
    "                )\n",
    "            self.batches.append(\n",
    "                (torch.stack(tuple(self._get_image_slice(self.images[len(self.images) - self.batch_size + j]) for j in range(self.batch_size))),\n",
    "                torch.stack(tuple(self._get_mask_slice(self.masks[len(self.masks) - self.batch_size + j]) for j in range(self.batch_size))))\n",
    "            )\n",
    "        return self.batches\n",
    "\n",
    "    def _get_image_slice(self, pos):\n",
    "        posy, posx = pos\n",
    "        dh, dw = self.kernel_size\n",
    "        return self.big_image[:, posy:posy+dh, posx:posx+dw]\n",
    "\n",
    "    def _get_mask_slice(self, pos):\n",
    "        posy, posx = pos\n",
    "        dh, dw = self.kernel_size\n",
    "        return self.big_mask[posy:posy+dh, posx:posx+dw]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.batches[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def merge_masks(self, masks):\n",
    "        answer = torch.zeros(size=(2, *self.orig_shape)).to(device)\n",
    "        counter = torch.zeros(size=(2, *self.orig_shape)).to(device)\n",
    "        k = 0\n",
    "        for batch in masks[:-1]:\n",
    "            for mask in batch:\n",
    "                posy, posx = self.images[k]\n",
    "                dh, dw = self.kernel_size\n",
    "                answer[:, posy: posy + dh, posx: posx + dw] += mask\n",
    "                counter[:, posy: posy + dh, posx: posx + dw] += 1\n",
    "                k += 1\n",
    "        k = len(self.images) - self.batch_size\n",
    "        for mask in masks[-1]:\n",
    "            posy, posx = self.images[k]\n",
    "            dh, dw = self.kernel_size\n",
    "            answer[:, posy: posy + dh, posx: posx + dw] += mask\n",
    "            counter[:, posy: posy + dh, posx: posx + dw] += 1\n",
    "            k += 1\n",
    "        answer = answer / counter\n",
    "        answer = torch.argmax(answer, dim = 0, keepdims=True)\n",
    "        # Convert to HWC\n",
    "        answer = answer.permute(1, 2, 0)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "\n",
    "filenames = [(f\"train/images/train_image_{i:03d}.png\", f\"train/masks/train_mask_{i:03d}.png\") for i in range(21)]\n",
    "random.shuffle(filenames)\n",
    "\n",
    "train_size = int(0.8 * len(filenames))\n",
    "\n",
    "train_filenames = filenames[:train_size]\n",
    "val_filenames = filenames[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ensemble import EnsembleModel\n",
    "\n",
    "model = EnsembleModel([\n",
    "    smp.UnetPlusPlus('resnet50', encoder_weights='imagenet', activation=None, classes = 2, encoder_depth= 5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "    smp.UnetPlusPlus('efficientnet-b7', encoder_weights='imagenet', activation=None, classes = 2, encoder_depth= 5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DiceLoss import DiceLoss\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "criterion2 = DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'F1 score': [],\n",
    "}\n",
    "\n",
    "def train_model(model, train_filenames, val_filenames, device, optimizer, epochs=1, batch_size=2):\n",
    "    best_score = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        loss_len = 0\n",
    "        with tqdm(\n",
    "            desc=f\"Epoch {epoch}/{epochs}\", unit=\"img\"\n",
    "        ) as pbar:\n",
    "            for im, mk in train_filenames:\n",
    "                train_loader = BatchesFromSingleImageMask(im, batch_size, mk, mode='train')\n",
    "                for i, batch in enumerate(train_loader):\n",
    "                    images, true_masks = batch\n",
    "        \n",
    "                    images, true_masks = images.to(device), true_masks.to(device)\n",
    "                    true_masks = true_masks.squeeze(dim=1)\n",
    "                    \n",
    "                    masks_pred = model(images)\n",
    "                    loss = criterion1(masks_pred, true_masks.long()) + criterion2(masks_pred, true_masks)\n",
    "                    epoch_loss += loss.item()\n",
    "                    loss_len += 1\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    pbar.update(images.shape[0])\n",
    "                    pbar.set_description(f\"Epoch: {epoch}, train_loss: {epoch_loss / (loss_len)}\")\n",
    "\n",
    "                del train_loader\n",
    "                gc.collect()\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            \n",
    "                \n",
    "        model.eval()\n",
    "        f1_metrics = np.array([])\n",
    "        epoch_loss = 0\n",
    "        loss_len = 0\n",
    "        \n",
    "        with tqdm(desc=f\"Validation\", unit=\"img\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for im, mk in val_filenames:\n",
    "                    val_loader = BatchesFromSingleImageMask(im, batch_size, mk, mode='val')\n",
    "        \n",
    "                    predictions_batches = []\n",
    "                    \n",
    "                    \n",
    "                    for i, batch in enumerate(val_loader):\n",
    "                        images, true_masks = batch\n",
    "        \n",
    "                        images, true_masks = images.to(device), true_masks.to(device)\n",
    "                        true_masks = true_masks.squeeze(dim=1)\n",
    "                        \n",
    "                        masks_pred = model(images)\n",
    "                        \n",
    "                        loss = criterion1(masks_pred, true_masks.long()) + criterion2(masks_pred, true_masks)\n",
    "                        epoch_loss += loss.item()\n",
    "                        loss_len += 1\n",
    "                        \n",
    "                        predictions_batches.append(masks_pred)\n",
    "                        \n",
    "                        pbar.update(images.shape[0])\n",
    "                        pbar.set_description(f\"val_loss: {epoch_loss / (loss_len)}\")\n",
    "                    \n",
    "                    pred_full = val_loader.merge_masks(predictions_batches).permute(2, 0, 1)\n",
    "                    true_full = val_loader.big_mask[None,:,:]\n",
    "                    \n",
    "                    f1_metrics = np.append(f1_metrics, F1(true_full, pred_full).cpu().numpy())\n",
    "                    \n",
    "        F1_score = f1_metrics.mean()\n",
    "        print(f'F1 score: {F1_score}')\n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['F1 score'].append(F1_score)\n",
    "        \n",
    "        if F1_score > best_score:\n",
    "            best_score = F1_score\n",
    "            torch.save(model.state_dict(), \"best_so_far_aug.pt\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 0.6882905835774963: : 990img [06:47,  2.43img/s]\n",
      "val_loss: 0.606541684709373: : 320img [00:38,  8.37img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5679290592670441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_loss: 0.5587540461296084: : 990img [07:04,  2.33img/s]\n",
      "val_loss: 0.5304873357858014: : 320img [00:38,  8.34img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.628374433517456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train_loss: 0.5058067934013752: : 990img [06:40,  2.47img/s] \n",
      "val_loss: 0.5643281044045579: : 320img [00:37,  8.65img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5889375567436218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train_loss: 0.4738956952439339: : 990img [06:28,  2.55img/s] \n",
      "val_loss: 0.5198419503569311: : 320img [00:37,  8.42img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6086065649986268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train_loss: 0.4649295512483352: : 990img [06:30,  2.53img/s] \n",
      "val_loss: 0.4965428570916629: : 320img [00:37,  8.44img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6323824882507324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6, train_loss: 0.4356533020203275: : 990img [06:36,  2.50img/s] \n",
      "val_loss: 0.5232323742726294: : 320img [00:37,  8.45img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5944790065288543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7, train_loss: 0.42584239118309186: : 990img [06:32,  2.52img/s]\n",
      "val_loss: 0.4999337378805649: : 320img [00:39,  8.16img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6221292853355408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8, train_loss: 0.4134891376781249: : 990img [06:26,  2.56img/s] \n",
      "val_loss: 0.5197943367420521: : 320img [00:38,  8.33img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5996833741664886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, train_loss: 0.3985140827216321: : 990img [06:29,  2.54img/s] \n",
      "val_loss: 0.48328468794916263: : 320img [00:37,  8.57img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6413819968700409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10, train_loss: 0.38799427925924196: : 990img [06:33,  2.52img/s]\n",
      "val_loss: 0.5179565865139011: : 320img [00:39,  8.03img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5976580381393433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11, train_loss: 0.3821788575314456: : 990img [06:26,  2.56img/s] \n",
      "val_loss: 0.4984022482480214: : 320img [00:38,  8.29img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6133952915668488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12, train_loss: 0.3773578780680904: : 990img [06:32,  2.52img/s] \n",
      "val_loss: 0.4793238894475508: : 320img [00:39,  8.11img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.653177797794342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 13, train_loss: 0.36349499670146596: : 990img [06:28,  2.55img/s]\n",
      "val_loss: 0.47842920513357967: : 320img [00:38,  8.41img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6474604725837707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14, train_loss: 0.3566798657634545: : 990img [06:26,  2.56img/s] \n",
      "val_loss: 0.4734068131121603: : 320img [00:38,  8.27img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6496530473232269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15, train_loss: 0.36073310652611346: : 990img [06:25,  2.57img/s]\n",
      "val_loss: 0.4741500841997549: : 320img [00:37,  8.61img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6406899452209472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16, train_loss: 0.3575632401188186: : 990img [06:21,  2.59img/s] \n",
      "val_loss: 0.4620364788273946: : 320img [00:37,  8.44img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6751301288604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17, train_loss: 0.34487339909343667: : 990img [06:26,  2.56img/s]\n",
      "val_loss: 0.47633795775691395: : 320img [00:37,  8.60img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6390000283718109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18, train_loss: 0.3493386948496783: : 990img [06:32,  2.52img/s] \n",
      "val_loss: 0.49706369961922975: : 320img [00:38,  8.32img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6251762092113495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19, train_loss: 0.3703317877701883: : 990img [06:31,  2.53img/s] \n",
      "val_loss: 0.47468332023645415: : 320img [00:36,  8.74img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6329201102256775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20, train_loss: 0.33427898931167693: : 990img [06:33,  2.52img/s]\n",
      "val_loss: 0.47806369029005963: : 320img [00:39,  8.15img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6390278220176697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21, train_loss: 0.32521690155471783: : 990img [06:24,  2.58img/s]\n",
      "val_loss: 0.4811975087803148: : 320img [00:36,  8.73img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6387229025363922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22, train_loss: 0.31726477956616866: : 990img [06:22,  2.59img/s]\n",
      "val_loss: 0.4728074350350653: : 320img [00:40,  8.00img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6523137211799621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23, train_loss: 0.3125051067286874: : 990img [06:20,  2.60img/s] \n",
      "val_loss: 0.47214195628403105: : 320img [00:37,  8.61img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6437747359275818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24, train_loss: 0.3193789739107638: : 990img [06:18,  2.62img/s] \n",
      "val_loss: 0.5092589639798462: : 320img [00:36,  8.72img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6026652991771698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 25, train_loss: 0.317013299642948: : 990img [06:16,  2.63img/s]  \n",
      "val_loss: 0.48057056702728007: : 320img [00:41,  7.76img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6305722117424011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26, train_loss: 0.29579644496029367: : 990img [06:24,  2.57img/s]\n",
      "val_loss: 0.47686129082121625: : 320img [00:36,  8.71img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6453683555126191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 27, train_loss: 0.2933906232232329: : 990img [06:17,  2.62img/s] \n",
      "val_loss: 0.49727900998841507: : 320img [00:37,  8.58img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6445564329624176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 28, train_loss: 0.32407298021849285: : 990img [06:24,  2.58img/s]\n",
      "val_loss: 0.48637980670609976: : 320img [00:40,  7.89img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6142610728740692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 29, train_loss: 0.3085187513404467: : 990img [07:49,  2.11img/s] \n",
      "val_loss: 0.49356872092030246: : 320img [00:47,  6.77img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6182458221912384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30, train_loss: 0.29117528009298965: : 990img [08:03,  2.05img/s]\n",
      "val_loss: 0.5095457875766443: : 320img [00:44,  7.23img/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6011254638433456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model, train_filenames, val_filenames, device, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting F1 score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['F1 score'], label='F1 Score', marker='o', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleModel([\n",
    "    smp.UnetPlusPlus('resnet50', encoder_weights='imagenet', classes = 2, activation=None, encoder_depth= 5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "    smp.UnetPlusPlus('efficientnet-b7', encoder_weights='imagenet', classes = 2, activation=None, encoder_depth= 5, decoder_channels=[256, 128, 64, 32, 16]),\n",
    "])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_unetplusplus_ensemble.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Post-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_small_buildings(image, min_size):\n",
    "    # Find connected components in the image\n",
    "    _, labels, stats, _ = cv2.connectedComponentsWithStats(image, connectivity=8)\n",
    "\n",
    "    # Iterate through connected components and remove small ones\n",
    "    for i in range(1, stats.shape[0]):  # Skip the background (label 0)\n",
    "        if stats[i, cv2.CC_STAT_AREA] < min_size:\n",
    "            image[labels == i] = 0\n",
    "\n",
    "    return torch.tensor(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(min_building_size, batch_size=3):\n",
    "    model.eval()\n",
    "    f1_metrics = np.array([])\n",
    "    \n",
    "    with tqdm(desc=f\"Validation\", unit=\"img\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for im, mk in val_filenames:\n",
    "                val_loader = BatchesFromSingleImageMask(im, batch_size, mk, mode='val')\n",
    "    \n",
    "                predictions_batches = []\n",
    "                \n",
    "                \n",
    "                for i, batch in enumerate(val_loader):\n",
    "                    images, true_masks = batch\n",
    "    \n",
    "                    images, true_masks = images.to(device), true_masks.to(device)\n",
    "                    true_masks = true_masks.squeeze(dim=1)\n",
    "                    \n",
    "                    masks_pred = model(images)\n",
    "                    \n",
    "                    predictions_batches.append(masks_pred)\n",
    "                    \n",
    "                    pbar.update(images.shape[0])\n",
    "                \n",
    "                pred_full = val_loader.merge_masks(predictions_batches).permute(2, 0, 1)\n",
    "                true_full = val_loader.big_mask[None,:,:]\n",
    "                \n",
    "                pred_full = delete_small_buildings(pred_full.squeeze(0).cpu().numpy().astype(np.uint8), min_building_size)\n",
    "                pred_full = pred_full.to(device)\n",
    "                f1_metrics = np.append(f1_metrics, F1(true_full, pred_full).cpu().numpy())\n",
    "                \n",
    "    F1_score = f1_metrics.mean()\n",
    "    print(f'F1 score: {F1_score}')\n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_size=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [00:45,  8.49img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7006216526031495\n",
      "min_size=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [00:51,  7.46img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7006275296211243\n",
      "min_size=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [00:55,  7.01img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.700641417503357\n",
      "min_size=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:03,  6.13img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7006716370582581\n",
      "min_size=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:05,  5.87img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7006911873817444\n",
      "min_size=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:14,  5.20img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.700722336769104\n",
      "min_size=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:17,  5.02img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7007508873939514\n",
      "min_size=70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:21,  4.76img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7008113622665405\n",
      "min_size=80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:24,  4.60img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.700816011428833\n",
      "min_size=90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:27,  4.45img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7008845090866089\n",
      "min_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:31,  4.21img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7009042143821717\n",
      "min_size=110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:35,  4.06img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.700953733921051\n",
      "min_size=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:34,  4.08img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.700960659980774\n",
      "min_size=130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:48,  3.56img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7009643316268921\n",
      "min_size=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:44,  3.70img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7010008454322815\n",
      "min_size=150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:27,  2.62img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7009868860244751\n",
      "min_size=160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:26,  2.64img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7010480523109436\n",
      "min_size=170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:07,  3.04img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7010757207870484\n",
      "min_size=180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:12,  2.92img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7011131286621094\n",
      "min_size=190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:53,  3.41img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7012294650077819\n",
      "min_size=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [01:56,  3.32img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7013792276382447\n",
      "min_size=210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:00,  3.20img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7014265656471252\n",
      "min_size=220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:12,  2.91img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7014810919761658\n",
      "min_size=230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:13,  2.89img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7014584422111512\n",
      "min_size=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:11,  2.93img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7015440702438355\n",
      "min_size=250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:20,  2.76img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.701536238193512\n",
      "min_size=260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:12,  2.92img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7014490246772767\n",
      "min_size=270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:17,  2.81img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7013948440551758\n",
      "min_size=280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:19,  2.78img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7013757228851318\n",
      "min_size=290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:32,  2.54img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7012992620468139\n",
      "min_size=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 387img [02:21,  2.74img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7012162923812866\n",
      "best_f1_score=0.7015440702438355, best_min_size=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_building_size = np.linspace(0, 300, num=31, dtype=np.int32)\n",
    "best_f1_score, best_min_size = 0, 0\n",
    "\n",
    "for min_size in min_building_size:\n",
    "    print(f'{min_size=}')\n",
    "    score = validation(min_size)\n",
    "    if score > best_f1_score:\n",
    "        best_f1_score = score\n",
    "        best_min_size = min_size\n",
    "print(f'{best_f1_score=}, {best_min_size=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch_size = 1\n",
    "test_filenames = [f\"test/images/test_image_{i:03d}.png\" for i in range(21)]\n",
    "\n",
    "with tqdm(desc=f\"Inference\", unit=\"img\") as pbar:\n",
    "    with torch.no_grad():\n",
    "        for j, im in enumerate(test_filenames):\n",
    "            test_loader = BatchesFromSingleImageMask(im, batch_size, None, test_mode=True)\n",
    "            \n",
    "            predictions_batches = []\n",
    "            \n",
    "            for i, images in enumerate(test_loader.generate_batches()):\n",
    "                images = images.to(device)\n",
    "                masks_pred = model(images)\n",
    "\n",
    "                pred = torch.argmax(masks_pred, dim=1)\n",
    "            \n",
    "                predictions_batches.append(masks_pred)\n",
    "                \n",
    "                pbar.update(images.shape[0])\n",
    "            pred_full = test_loader.merge_masks(predictions_batches).permute(2, 0, 1)\n",
    "            np_pred_full = pred_full.squeeze(0).cpu().numpy()\n",
    "            plt.imsave(f'test/masks/test_image_{j:03d}.png', np_pred_full)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(original_image, prediction_mask, true_mask):\n",
    "    original_image = cv2.imread(original_image cv2.IMREAD_COLOR)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    prediction_mask = prediction_mask.cpu().numpy()\n",
    "    true_mask = true_mask.cpu().numpy()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(original_image)\n",
    "    plt.imshow(true_mask, alpha=0.6)\n",
    "    plt.title('Original mask')\n",
    "    plt.show()\n",
    "    # fig.savefig('figure_real.png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(original_image)\n",
    "    plt.imshow(prediction_maskk, alpha=0.6)\n",
    "    plt.title('Prediction mask')\n",
    "    plt.show()\n",
    "    # fig.savefig('figure_predicted.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
